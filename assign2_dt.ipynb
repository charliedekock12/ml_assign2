{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charles de Kock - 26023830 - ML441 Assignment 2\n",
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "import json\n",
    "import itertools\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_og = pd.read_csv(os.path.dirname(os.getcwd()) + '/forestCover.csv', na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dt_preprocessing(df: pd.DataFrame):\n",
    "\n",
    "    df = df.drop(columns=['Inclination', 'Observation_ID'])\n",
    "    #df['Soil_Type1'] = df['Soil_Type1'].map({'positive': 1, 'negative': 0}).astype(np.int8)\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = init_dt_preprocessing(data_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing_transform(df: pd.DataFrame, target, strategy_over, strategy_under, r=None):\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    under = RandomUnderSampler(sampling_strategy=strategy_under, random_state=r)\n",
    "    X_under, y_under = under.fit_resample(X, y)\n",
    "\n",
    "    smote = SMOTE(sampling_strategy=strategy_over, random_state=r+1)\n",
    "    #smote_tomek = SMOTETomek(sampling_strategy=strategy_over, random_state=r) #consider using later, takes long tho\n",
    "    X_res, y_res = smote.fit_resample(X_under, y_under)\n",
    "\n",
    "    df_balanced = pd.DataFrame(X_res, columns=X.columns)\n",
    "    df_balanced[target] = y_res\n",
    "\n",
    "    return df_balanced\n",
    "\n",
    "def balancing_transform_lean(X, y, strategy_over, strategy_under, r=None):\n",
    "\n",
    "    under = RandomUnderSampler(sampling_strategy=strategy_under, random_state=r)\n",
    "    X_under, y_under = under.fit_resample(X, y)\n",
    "\n",
    "    smote = SMOTE(sampling_strategy=strategy_over, random_state=r)\n",
    "    X_res, y_res = smote.fit_resample(X_under, y_under)\n",
    "\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean = data_clean.drop(columns=['Cover_Type'])\n",
    "y_clean = data_clean['Cover_Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Cross fold setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)\n",
    "\n",
    "folds = []\n",
    "\n",
    "for train_index, val_index in skf.split(X_clean, y_clean):\n",
    "    X_train, X_val = X_clean.iloc[train_index], X_clean.iloc[val_index]\n",
    "    y_train, y_val = y_clean.iloc[train_index], y_clean.iloc[val_index]\n",
    "    folds.append({'Xt': X_train, 'Xv': X_val, 'yt': y_train, 'yv': y_val})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_fold(fold, strategies):\n",
    "    X, y = balancing_transform_lean(fold['Xt'], fold['yt'], strategies[1], strategies[0])\n",
    "    return {'Xt': X, 'Xv': fold['Xv'], 'yt': y, 'yv': fold['yv']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_A = [{1: 50000, 2: 50000}, {3: 35000, 4: 20000, 5: 20000, 6: 30000, 7: 30000}]\n",
    "\n",
    "for i in tqdm(range(len(folds))):\n",
    "    folds[i] = oversample_fold(folds[i], strategies=strat_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(fold, fold_nr, param_id, params):\n",
    "    model = DecisionTreeClassifier(criterion=params['criterion'],\n",
    "                                   max_depth=params['max_depth'], \n",
    "                                   min_samples_leaf=params['min_samples_leaf'], \n",
    "                                   class_weight=params['class_weight'])\n",
    "    \n",
    "    model.fit(fold['Xt'], fold['yt'])\n",
    "    y_pred = model.predict(fold['Xv'])\n",
    "    f1_w = f1_score(fold['yv'], y_pred, average='weighted')\n",
    "    f1_m = f1_score(fold['yv'], y_pred, average='macro')\n",
    "    mcc = matthews_corrcoef(fold['yv'], y_pred)\n",
    "    \n",
    "    return {'fold_nr': fold_nr, 'param_id': param_id,'params': params, 'mcc': mcc, 'f1_weigthed': f1_w, 'f1_macro': f1_m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_param_dicts(criteria, depths, min_samples_leaf, weight):\n",
    "    param_dicts = []\n",
    "    for c, d, ms, w in itertools.product(criteria, depths, min_samples_leaf, weight):\n",
    "            params = {\n",
    "                'criterion': c,\n",
    "                'max_depth': d, \n",
    "                'min_samples_leaf': ms, \n",
    "                'class_weight': w\n",
    "            }\n",
    "            param_dicts.append(params)\n",
    "    return param_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(param_grid, folds):\n",
    "    out = []\n",
    "    idx = 0\n",
    "    for i in tqdm(range(len(param_grid))):\n",
    "        for j in range(len(folds)):\n",
    "            out.append(eval_model(folds[j], j, i, param_grid[i]))\n",
    "            #print(\"Params:\", i, '\\nFold:', j, '\\n')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, file):\n",
    "    with open(file, \"w\") as f:\n",
    "        for d in results:\n",
    "            f.write(json.dumps(d) + \"\\n\")\n",
    "\n",
    "def load_results(file):\n",
    "    loaded_results = []\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            loaded_results.append(json.loads(line))\n",
    "    return loaded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(20, 50, 12).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = generate_param_dicts(['gini', 'entropy'], np.linspace(20, 50, 12).astype(int), [1, 0.0002], [None, 'balanced'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fine = grid_search(param_grid, folds)\n",
    "save_results(results_fine, \"eval_fine_dt.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results_fine:\n",
    "    result['params']['max_depth'] = int(result['params']['max_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(results_fine, \"eval_fine_dt_backup.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_results(data):\n",
    "    grouped_data = defaultdict(list)\n",
    "    for d in data:\n",
    "        grouped_data[d['param_id']].append(d)\n",
    "\n",
    "    summary_list = []\n",
    "    for param_id, group in grouped_data.items():\n",
    "        mcc_scores = [d['mcc'] for d in group]\n",
    "        f1_weigthed_scores = [d['f1_weigthed'] for d in group]\n",
    "        f1_macro_scores = [d['f1_macro'] for d in group]\n",
    "\n",
    "        summary = {\n",
    "            'param_id': param_id,\n",
    "            'params': group[0]['params'],\n",
    "            'mean_mcc': np.mean(mcc_scores),\n",
    "            'std_mcc': np.std(mcc_scores),\n",
    "            'mean_f1_weigthed': np.mean(f1_weigthed_scores),\n",
    "            'std_f1_weigthed': np.std(f1_weigthed_scores),\n",
    "            'mean_f1_macro': np.mean(f1_macro_scores),\n",
    "            'std_f1_macro': np.std(f1_macro_scores)\n",
    "        }\n",
    "        summary_list.append(summary)\n",
    "\n",
    "    return summary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_results(results, key, show=False):\n",
    "    out = sorted(results, key=lambda x: x['mean_' + key], reverse=True)\n",
    "    if show == True:\n",
    "        for r in out:\n",
    "            print(f\"ID: {r['param_id']} Parameters: {r['params']} -> {key}: {r['mean_' + key]} ± {r['std_' + key]}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_fine_evals_mcc = sort_results(avg_results(results_fine), 'mcc', show=False)\n",
    "sorted_fine_evals_f1w = sort_results(avg_results(results_fine), 'f1_weigthed', show=False)\n",
    "sorted_fine_evals_f1m = sort_results(avg_results(results_fine), 'f1_macro', show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_evals(eval1, eval2, eval3):\n",
    "    out = defaultdict(int)\n",
    "    for i in range(len(eval1)):\n",
    "        out[str(eval1[i]['param_id'])] += (i + 1)\n",
    "    for i in range(len(eval2)):\n",
    "        out[str(eval2[i]['param_id'])] += (i + 1)\n",
    "    for i in range(len(eval3)):\n",
    "        out[str(eval3[i]['param_id'])] += (i + 1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ranks = combine_evals(sorted_fine_evals_mcc, sorted_fine_evals_f1w, sorted_fine_evals_f1m)\n",
    "sorted(combined_ranks.items(), key=lambda item: item[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ID(results , id):\n",
    "    return [d for d in results if d.get('param_id') == id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = find_ID(results_fine, 183)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
